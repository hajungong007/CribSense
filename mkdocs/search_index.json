{
    "docs": [
        {
            "location": "/",
            "text": "CribSense Overview\n\n\nCribSense is an video-based, contactless baby monitor that you can make yourself without breaking the bank.\n\n\n\n\nCribSense is a C++ implementation of \nVideo Magnification\n tuned to run on a Raspberry Pi 3 Model B. Over a weekend, you can setup your own crib-side baby monitor that raises an alarm if your infant stops moving. As a bonus all of the software is free to use for non-commercial purposes and easily extensible.\n\n\nTODO: Picture showing essence of finished project\n\n\nGetting Started\n\n\nCribSense is made up of two parts: the baby monitoring software and some simple hardware. Find out all you need to know about how to use and recreate your own monitor in the sections below.\n\n\nSoftware\n\n\nTo install the prerequisites, build the software, or learn more about the software architecture, see the \nSoftware Setup Guide\n\n\nHardware\n\n\nTo learn more about the hardware materials we used, and how we build our setup, check out the \nHardware Setup Guide",
            "title": "Home"
        },
        {
            "location": "/#cribsense-overview",
            "text": "CribSense is an video-based, contactless baby monitor that you can make yourself without breaking the bank.   CribSense is a C++ implementation of  Video Magnification  tuned to run on a Raspberry Pi 3 Model B. Over a weekend, you can setup your own crib-side baby monitor that raises an alarm if your infant stops moving. As a bonus all of the software is free to use for non-commercial purposes and easily extensible.  TODO: Picture showing essence of finished project",
            "title": "CribSense Overview"
        },
        {
            "location": "/#getting-started",
            "text": "CribSense is made up of two parts: the baby monitoring software and some simple hardware. Find out all you need to know about how to use and recreate your own monitor in the sections below.  Software  To install the prerequisites, build the software, or learn more about the software architecture, see the  Software Setup Guide  Hardware  To learn more about the hardware materials we used, and how we build our setup, check out the  Hardware Setup Guide",
            "title": "Getting Started"
        },
        {
            "location": "/setup/sw-setup/",
            "text": "Software Setup Guide\n\n\nThis is the step-by-step guide on building the cribsense software on Ubuntu.\n\n\nPrerequisites\n\n\nThis software depends on autoconf, OpenCV and libcanberra. Install these by running\n\n\nsudo apt-get install git build-essential autoconf libopencv-dev libcanberra-dev\n\n\n\n\nBuild\n\n\nTo build the software, navigate to the root of the repository directory and run\n\n\n./autogen.sh --prefix=/usr --sysconfdir=/etc --disable-debug\nmake\nsudo make install\nsudo systemctl daemon-reload\n\n\n\n\nTo start the program in the background:\n\n\nsudo systemctl start cribsense\n\n\n\n\nTo run it in the foreground:\n\n\ncribsense --config /etc/cribsense/config.ini\n\n\n\n\nTo start the program automatically at every boot:\n\n\nsudo systemctl enable cribsense\n\n\n\n\nTo stop cribsense from automatically running at boot:\n\n\nsudo systemctl disable cribsense\n\n\n\n\nNote that when \ncribsense\n is started using \nsystemctl\n, the config parameters are already sent and are stored in \n/etc/systemd/system/cribsense.service\n\n\nSoftware Configuration\n\n\nCribSense customizable through a simple INI configuration file. After running \nmake install\n, the configuration file is located at:\n\n\nsudo nano /etc/cribsense/config.ini\n\n\n\n\nTODO: Detail what each of the configuration parameters does.\n\n\nTODO: Link to video describing the calibration process.\n\n\nSoftware Architecture Details\n\n\nThe software for processing a video stream is implemented as a state machine that cycles as shown below. There are currently six states that manage all of our processing steps. Our state machine logic is called each time a new frame is read from the camera.\n\n\n\n\nInitialization\n\n\nWhen the video stream first turns on, it is common to have a flash of white or black pixels that looks like a lot of motion is happening. This state simply initializes the video magnification and motion detection code, and skips a few of the first frames before jumping into monitoring motion.\n\n\nMonitoring Motion\n\n\nIn this state, the full 640 x 480 frame is magnified, and uses an image differential algorithm from Collins et al. to calculate the motion pixels between frames. The output of this algorithm is a black and white image where white pixels indicate pixels that have changed. These black and white images are then bitwise ORed together for several frames to accumulate the motion seen over the period of time.\n\n\nComputing a Region of Interest\n\n\nWith the accumulated black and white frame representing the motion seen over several frames, the image is eroded slightly to eliminate noise, then significantly dilated to highlight the areas with the most motion. Dilation is necessary to merge discrete points into continuous regions of motion. Because of the large dilation, it is then easy to find the largest contour in the black and white image, which represents the main source of motion, and draw a bounding box around the contour. It is in this portion of code that we can set bounds on the size of the crop.\n\n\nSteady State\n\n\nThis is where our software spends most of its time. In this state, the software is operating on cropped frames. Video magnification is performed and a measure of the movement seen in the frame is calculated. In this state, we are able to process at a fast enough rate to catch up with any delay caused during the processing of full frames, and keep up with our 10fps video stream.\n\n\nPeriodic Reset\n\n\nBecause the baby may move occasionally, we periodically reset the video magnification and cropping so that we have full-resolution frames available to repeat the process of monitoring motion and finding a new region of interest. All of these timing parameters are easily configurable.\n\n\nTechnical Challenges\n\n\nSpeed\n\n\nWhen we started this project, our C++ implementation could magnify video at a rate of 394,000 pixels per second on the Raspberry Pi 3. For reference, this meant our code could only process a 640 x 480 video stream at a rate of approximately 1.3 frames per second.  Optimizing our code to handle the real-time goal was our primary objective and challenge, since we needed a 10x speed-up to handle at least 10 frames per second. Now, we are able to process over 1,200,000 pixels per second, and use additional cropping to process a 10 fps video stream in real time.\nWe used three main optimizations increase performance by 10x: (1) multithreading, (2) adaptive cropping, and (3) compiler optimization. Each optimization is explained in more detail below.\n\n\nMultithreading (~3x)\n\n\nOne intuitive and high-value optimization was to process sections of each video frame in parallel. To do so, we divide each frame vertically, such that each section's height is one-third of the original frame. Then, each section is magnified as if it was its own video stream. The resulting magnified frames are then joined together to form a full-resolution frame before evaluating the frame for motion. This optimization alone brought our processing rate to 938,000 pixels per second (roughly a 3x improvement). Three of the Raspberry Pi's four cores are dedicated to video magnification, while the 4th core handles control flow and motion processing.\n\n\nAdaptive Cropping (~3x)\n\n\nWhile multithreading sought to increase the number of pixels we could process each second, adaptive cropping sought to decrease the number of pixels we needed to process. We rely on two main assumptions for this optimization. First, the camera captures the entire crib in its field of view and the baby is the only source of movement in the frame.  Second, the camera is placed such that movement is discernible, and the region of interest is not larger than one-third of the frame. With these assumptions, we are able to periodically monitor a full frame to determine where the motion is occurring, then crop down the stream to only contain the relevant motion. By reducing the number of pixels that we process in our steady state, and amortizing the cost of monitoring a few full-resolution frames, we are able to increase the number of frames we can handle each second by about 3x.\n\n\nCompiler Optimizations (~1.5x)\n\n\nFinally, in order to reach our goal of over 10x speedup, we were able to make some code optimizations and utilize additional compiler flags. In particular, we make use of the -O3 optimization level, and we force the compiler to generate vector instructions with -ftree-vectorize and -mfpu=crypto-neon-fp-armv8. Additionally, we were required to add the -funsafe-math-optimization flag to enable the vector float generation, because NEON instructions are not IEEE compliant and gcc would not generate them otherwise.\nWe found two major challenges in enabling compiler optimization. The first was to expose the operations to the compiler. The most expensive part of transformation happens in two all-pixel loops, but previously each operation in the loop was implemented with an opencv call that would run the operation over all pixels. We rewrote the code to pull out the loop and avoid intermediate copies. On an x86-64 machine, this is already an improvement, because of larger caches and the fact that the compiler always issues at least SSE2 vector instructions. In contrast, the switch from vector instruction enabled opencv to straight C++ code resulted in worse performance on the Pi.\nWe recovered this performance drop by adding the compiler flags, but we encountered toolchain problems. The Pi 3 has an ARM Cortex A53 processor, which is an A-class 64-bit ARMv8 with NEON and crypto extensions, but it runs a 32-bit OS compiled for ARMv7. We found first that compiling with -march=native exposes a known but yet unfixed GCC bug. Compiling with -march=armv8 appears to work, but generates ABI incompatible binaries when linked with an ARMv7 libstdc++, leading to stack corruption. On the other hand, leaving the default of -march=armv7 and forcing just the FPU to generate NEON appears to work, looking at the assembly. We also added -mtune=cortex-a53 to achieve better instruction scheduling and better counts for loop unrolling (which is enabled at -O3), but we suspect it has no effect because the version of GCC we use (4.9.2, provided by Raspbian) does not recognize it.\n\n\nProfiling\n\n\nIn order to isolate further optimization spots, we employed profiling to identify where the code is spending most of the time.\nIn doing so, we also encountered toolchain problems, as valgrind is unable to emulate the NEON vector instructions and crashes. We therefore turned to gprof, but we found its timing somewhat unreliable, despite the -fno-inline switch (function call counts on the other hand are precise but not quite useful). Finally, we tried perf, which uses the HW performance counters and gives us instruction level timings, but we found that it is very precise inside the transformation code, and has zero data outside, because it sees no sample of the other code. Overall, profiling suggests that 96% of the time is spent in the transformation and 2% in motion detection, with the rest being threading and locking overhead, and accessing the camera.\n\n\nTiming\n\n\nOverall, our algorithm is able to process a full 640x480 frame in 220 to 240 ms, while after cropping our algorithm runs in the worst case (where the crop is approximately 320x320) at about 70 ms per frame. This allows us to maintain a steady 10 fps goal.",
            "title": "Software Setup Instructions"
        },
        {
            "location": "/setup/sw-setup/#software-setup-guide",
            "text": "This is the step-by-step guide on building the cribsense software on Ubuntu.",
            "title": "Software Setup Guide"
        },
        {
            "location": "/setup/sw-setup/#prerequisites",
            "text": "This software depends on autoconf, OpenCV and libcanberra. Install these by running  sudo apt-get install git build-essential autoconf libopencv-dev libcanberra-dev",
            "title": "Prerequisites"
        },
        {
            "location": "/setup/sw-setup/#build",
            "text": "To build the software, navigate to the root of the repository directory and run  ./autogen.sh --prefix=/usr --sysconfdir=/etc --disable-debug\nmake\nsudo make install\nsudo systemctl daemon-reload  To start the program in the background:  sudo systemctl start cribsense  To run it in the foreground:  cribsense --config /etc/cribsense/config.ini  To start the program automatically at every boot:  sudo systemctl enable cribsense  To stop cribsense from automatically running at boot:  sudo systemctl disable cribsense  Note that when  cribsense  is started using  systemctl , the config parameters are already sent and are stored in  /etc/systemd/system/cribsense.service",
            "title": "Build"
        },
        {
            "location": "/setup/sw-setup/#software-configuration",
            "text": "CribSense customizable through a simple INI configuration file. After running  make install , the configuration file is located at:  sudo nano /etc/cribsense/config.ini  TODO: Detail what each of the configuration parameters does.  TODO: Link to video describing the calibration process.",
            "title": "Software Configuration"
        },
        {
            "location": "/setup/sw-setup/#software-architecture-details",
            "text": "The software for processing a video stream is implemented as a state machine that cycles as shown below. There are currently six states that manage all of our processing steps. Our state machine logic is called each time a new frame is read from the camera.   Initialization  When the video stream first turns on, it is common to have a flash of white or black pixels that looks like a lot of motion is happening. This state simply initializes the video magnification and motion detection code, and skips a few of the first frames before jumping into monitoring motion.  Monitoring Motion  In this state, the full 640 x 480 frame is magnified, and uses an image differential algorithm from Collins et al. to calculate the motion pixels between frames. The output of this algorithm is a black and white image where white pixels indicate pixels that have changed. These black and white images are then bitwise ORed together for several frames to accumulate the motion seen over the period of time.  Computing a Region of Interest  With the accumulated black and white frame representing the motion seen over several frames, the image is eroded slightly to eliminate noise, then significantly dilated to highlight the areas with the most motion. Dilation is necessary to merge discrete points into continuous regions of motion. Because of the large dilation, it is then easy to find the largest contour in the black and white image, which represents the main source of motion, and draw a bounding box around the contour. It is in this portion of code that we can set bounds on the size of the crop.  Steady State  This is where our software spends most of its time. In this state, the software is operating on cropped frames. Video magnification is performed and a measure of the movement seen in the frame is calculated. In this state, we are able to process at a fast enough rate to catch up with any delay caused during the processing of full frames, and keep up with our 10fps video stream.  Periodic Reset  Because the baby may move occasionally, we periodically reset the video magnification and cropping so that we have full-resolution frames available to repeat the process of monitoring motion and finding a new region of interest. All of these timing parameters are easily configurable.",
            "title": "Software Architecture Details"
        },
        {
            "location": "/setup/sw-setup/#technical-challenges",
            "text": "Speed  When we started this project, our C++ implementation could magnify video at a rate of 394,000 pixels per second on the Raspberry Pi 3. For reference, this meant our code could only process a 640 x 480 video stream at a rate of approximately 1.3 frames per second.  Optimizing our code to handle the real-time goal was our primary objective and challenge, since we needed a 10x speed-up to handle at least 10 frames per second. Now, we are able to process over 1,200,000 pixels per second, and use additional cropping to process a 10 fps video stream in real time.\nWe used three main optimizations increase performance by 10x: (1) multithreading, (2) adaptive cropping, and (3) compiler optimization. Each optimization is explained in more detail below.  Multithreading (~3x)  One intuitive and high-value optimization was to process sections of each video frame in parallel. To do so, we divide each frame vertically, such that each section's height is one-third of the original frame. Then, each section is magnified as if it was its own video stream. The resulting magnified frames are then joined together to form a full-resolution frame before evaluating the frame for motion. This optimization alone brought our processing rate to 938,000 pixels per second (roughly a 3x improvement). Three of the Raspberry Pi's four cores are dedicated to video magnification, while the 4th core handles control flow and motion processing.  Adaptive Cropping (~3x)  While multithreading sought to increase the number of pixels we could process each second, adaptive cropping sought to decrease the number of pixels we needed to process. We rely on two main assumptions for this optimization. First, the camera captures the entire crib in its field of view and the baby is the only source of movement in the frame.  Second, the camera is placed such that movement is discernible, and the region of interest is not larger than one-third of the frame. With these assumptions, we are able to periodically monitor a full frame to determine where the motion is occurring, then crop down the stream to only contain the relevant motion. By reducing the number of pixels that we process in our steady state, and amortizing the cost of monitoring a few full-resolution frames, we are able to increase the number of frames we can handle each second by about 3x.  Compiler Optimizations (~1.5x)  Finally, in order to reach our goal of over 10x speedup, we were able to make some code optimizations and utilize additional compiler flags. In particular, we make use of the -O3 optimization level, and we force the compiler to generate vector instructions with -ftree-vectorize and -mfpu=crypto-neon-fp-armv8. Additionally, we were required to add the -funsafe-math-optimization flag to enable the vector float generation, because NEON instructions are not IEEE compliant and gcc would not generate them otherwise.\nWe found two major challenges in enabling compiler optimization. The first was to expose the operations to the compiler. The most expensive part of transformation happens in two all-pixel loops, but previously each operation in the loop was implemented with an opencv call that would run the operation over all pixels. We rewrote the code to pull out the loop and avoid intermediate copies. On an x86-64 machine, this is already an improvement, because of larger caches and the fact that the compiler always issues at least SSE2 vector instructions. In contrast, the switch from vector instruction enabled opencv to straight C++ code resulted in worse performance on the Pi.\nWe recovered this performance drop by adding the compiler flags, but we encountered toolchain problems. The Pi 3 has an ARM Cortex A53 processor, which is an A-class 64-bit ARMv8 with NEON and crypto extensions, but it runs a 32-bit OS compiled for ARMv7. We found first that compiling with -march=native exposes a known but yet unfixed GCC bug. Compiling with -march=armv8 appears to work, but generates ABI incompatible binaries when linked with an ARMv7 libstdc++, leading to stack corruption. On the other hand, leaving the default of -march=armv7 and forcing just the FPU to generate NEON appears to work, looking at the assembly. We also added -mtune=cortex-a53 to achieve better instruction scheduling and better counts for loop unrolling (which is enabled at -O3), but we suspect it has no effect because the version of GCC we use (4.9.2, provided by Raspbian) does not recognize it.  Profiling  In order to isolate further optimization spots, we employed profiling to identify where the code is spending most of the time.\nIn doing so, we also encountered toolchain problems, as valgrind is unable to emulate the NEON vector instructions and crashes. We therefore turned to gprof, but we found its timing somewhat unreliable, despite the -fno-inline switch (function call counts on the other hand are precise but not quite useful). Finally, we tried perf, which uses the HW performance counters and gives us instruction level timings, but we found that it is very precise inside the transformation code, and has zero data outside, because it sees no sample of the other code. Overall, profiling suggests that 96% of the time is spent in the transformation and 2% in motion detection, with the rest being threading and locking overhead, and accessing the camera.  Timing  Overall, our algorithm is able to process a full 640x480 frame in 220 to 240 ms, while after cropping our algorithm runs in the worst case (where the crop is approximately 320x320) at about 70 ms per frame. This allows us to maintain a steady 10 fps goal.",
            "title": "Technical Challenges"
        },
        {
            "location": "/setup/hw-setup/",
            "text": "Hardare Setup Guide\n\n\n\n\nFigure TODO: Hardware Block Diagram\n\n\nCribSense is relatively simple as far as hardware goes, and is largely made up of commercially available items. As seen in Figure TODO, there are only 5 main hardware components, and only 2 of them are custom made. This page will walk through how to construct the IR LED circuit, and the chassis we used.\n\n\nWhat you'll need\n\n\n\n\nRaspberry Pi 3 Model B\n\n\n5V 2.5A Micro USB Power Supply\n\n\nRaspberry Pi NoIR Camera Module V2\n\n\n1W IR LED\n\n\nMicroSD Card\n (we went with a class 10 16GB card, the faster the card the better)\n\n\nFlex Cable for Raspberry Pi Camera (12\")\n\n\n[3x] 1N4001 diodes\n\n\n1 ohm, 1W resistor\n\n\n[2x] 12\" Wires with pin headers\n\n\nSoldering iron\n\n\n3D printer for chassis\n\n\nSpeakers with 3.5mm input\n\n\nHDMI monitor\n\n\nKeyboard\n\n\nMouse\n\n\n\n\nPrerequisites\n\n\nBefore you start our step-by-step guide, you should have already installed the latest version of \nRaspbian\n on your SD card and ensured that your Pi is functional and booting. You'll also need to \nenable the camera module\n before you'll be able to interface with the camera.\n\n\nBuild Instructions\n\n\n3D Printed Chassis\n\n\nTODO: William to fill in instructions about how to use the source files to print out the chassis (or links to good tutorials)\n\n\nTODO: Build process along with pictures.\n\n\nIR LED Circuit\n\n\nIn order to provide adequate lighting at night, we use an IR LED, which is not visible to the human eye, but brightly illuminating for our NoIR camera. Because the Pi is plugged in, and because the LED is relatively low power, we just leave it on for simplicity.\n\n\nTo power the LED from the GPIO header pins on the Pi, we construct the circuit in Figure TODO.\n\n\n\n\nFigure TODO: LED Schematic\n\n\nIn earlier versions of the Pi, the maximum current output of these pins was \n50mA\n. The Raspberry Pi B+ increased this to 500mA. However, for simplicity and backwards compatibility, we just use the 5V power pins, which can supply up to \n1.5A\n. The forward voltage of the IR LED is about 1.7~1.9V according to our measurements. Although the IR LED can draw 500mA without damaging itself, we decided to reduce the current to around 200mA to reduce heat and overall power consumption. Experimental result also show that the IR LED is bright enough with this input. To bridge the gap between 5V and 1.9V, we decided to use three 1N4001 diodes and a 1 Ohm resistor in series with the IR LED. The voltage loss on the wire is about 0.2V, over the diodes it's 0.9V (for each one), and over the resistor it's 0.2V. So the voltage over the IR LED is \n5V - 0.2V - (3 * 0.9V) - 0.2V = 1.9V\n. The heat dissipation over each LED is 0.18W, and is 0.2W over the resistor, all well within the maximum ratings.\n\n\nThe completed circuit is pictured below:\n\n\nTODO: Add pictures of the actual circuit.",
            "title": "Hardware Setup Instructions"
        },
        {
            "location": "/setup/hw-setup/#hardare-setup-guide",
            "text": "Figure TODO: Hardware Block Diagram  CribSense is relatively simple as far as hardware goes, and is largely made up of commercially available items. As seen in Figure TODO, there are only 5 main hardware components, and only 2 of them are custom made. This page will walk through how to construct the IR LED circuit, and the chassis we used.",
            "title": "Hardare Setup Guide"
        },
        {
            "location": "/setup/hw-setup/#what-youll-need",
            "text": "Raspberry Pi 3 Model B  5V 2.5A Micro USB Power Supply  Raspberry Pi NoIR Camera Module V2  1W IR LED  MicroSD Card  (we went with a class 10 16GB card, the faster the card the better)  Flex Cable for Raspberry Pi Camera (12\")  [3x] 1N4001 diodes  1 ohm, 1W resistor  [2x] 12\" Wires with pin headers  Soldering iron  3D printer for chassis  Speakers with 3.5mm input  HDMI monitor  Keyboard  Mouse",
            "title": "What you'll need"
        },
        {
            "location": "/setup/hw-setup/#prerequisites",
            "text": "Before you start our step-by-step guide, you should have already installed the latest version of  Raspbian  on your SD card and ensured that your Pi is functional and booting. You'll also need to  enable the camera module  before you'll be able to interface with the camera.",
            "title": "Prerequisites"
        },
        {
            "location": "/setup/hw-setup/#build-instructions",
            "text": "3D Printed Chassis  TODO: William to fill in instructions about how to use the source files to print out the chassis (or links to good tutorials)  TODO: Build process along with pictures.  IR LED Circuit  In order to provide adequate lighting at night, we use an IR LED, which is not visible to the human eye, but brightly illuminating for our NoIR camera. Because the Pi is plugged in, and because the LED is relatively low power, we just leave it on for simplicity.  To power the LED from the GPIO header pins on the Pi, we construct the circuit in Figure TODO.   Figure TODO: LED Schematic  In earlier versions of the Pi, the maximum current output of these pins was  50mA . The Raspberry Pi B+ increased this to 500mA. However, for simplicity and backwards compatibility, we just use the 5V power pins, which can supply up to  1.5A . The forward voltage of the IR LED is about 1.7~1.9V according to our measurements. Although the IR LED can draw 500mA without damaging itself, we decided to reduce the current to around 200mA to reduce heat and overall power consumption. Experimental result also show that the IR LED is bright enough with this input. To bridge the gap between 5V and 1.9V, we decided to use three 1N4001 diodes and a 1 Ohm resistor in series with the IR LED. The voltage loss on the wire is about 0.2V, over the diodes it's 0.9V (for each one), and over the resistor it's 0.2V. So the voltage over the IR LED is  5V - 0.2V - (3 * 0.9V) - 0.2V = 1.9V . The heat dissipation over each LED is 0.18W, and is 0.2W over the resistor, all well within the maximum ratings.  The completed circuit is pictured below:  TODO: Add pictures of the actual circuit.",
            "title": "Build Instructions"
        }
    ]
}