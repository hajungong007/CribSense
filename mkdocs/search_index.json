{
    "docs": [
        {
            "location": "/",
            "text": "CribSense Overview\n\n\nCribSense is a contactless, video-based baby monitor that you can make yourself without breaking the bank.\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nCribSense is a C++ implementation of the MIT \nVideo Magnification\n algorithm that is tuned to run on a Raspberry Pi 3 Model B. Over a weekend, you can setup your own crib-side baby monitor that raises an alarm if your infant stops moving. As a bonus all of the software is free to use for non-commercial purposes and easily extensible.\n\n\nWhile we think that CribSense is pretty fun, it is important to remember that this is not actually a certified, foolproof safety device. That is, CribSense will not work as expected if the system is not calibrated properly. If the system is not calibrated well or the environment in the video is not conducive to video magnification, you may not be able to use it. We made this as a fun side project to see how well we could run compute-heavy software, like video magnification, on compute-limited hardware, like a Raspberry Pi. Any real product would require much more testing than we have done. If you use this project, take it for what it is: a short exploration of video magnification on a Raspberry Pi.\n\n\nGetting Started\n\n\nCribSense is made up of two parts: the baby monitoring software and some simple hardware. Find out all you need to know about how to use and recreate your own monitor in the sections below.\n\n\nSoftware\n\n\nTo install the prerequisites, build the software, or learn more about the software architecture, see the \nSoftware Setup Guide\n.\n\n\nHardware\n\n\nTo learn more about the hardware materials we used, and how we build our setup, check out the \nHardware Setup Guide\n.\n\n\nVideo Walkthroughs\n\n\nCalibrating CribSense\n\n\n\n\n\nCribSense Demonstration",
            "title": "Home"
        },
        {
            "location": "/#cribsense-overview",
            "text": "CribSense is a contactless, video-based baby monitor that you can make yourself without breaking the bank.  \n     \n     CribSense is a C++ implementation of the MIT  Video Magnification  algorithm that is tuned to run on a Raspberry Pi 3 Model B. Over a weekend, you can setup your own crib-side baby monitor that raises an alarm if your infant stops moving. As a bonus all of the software is free to use for non-commercial purposes and easily extensible.  While we think that CribSense is pretty fun, it is important to remember that this is not actually a certified, foolproof safety device. That is, CribSense will not work as expected if the system is not calibrated properly. If the system is not calibrated well or the environment in the video is not conducive to video magnification, you may not be able to use it. We made this as a fun side project to see how well we could run compute-heavy software, like video magnification, on compute-limited hardware, like a Raspberry Pi. Any real product would require much more testing than we have done. If you use this project, take it for what it is: a short exploration of video magnification on a Raspberry Pi.",
            "title": "CribSense Overview"
        },
        {
            "location": "/#getting-started",
            "text": "CribSense is made up of two parts: the baby monitoring software and some simple hardware. Find out all you need to know about how to use and recreate your own monitor in the sections below.  Software  To install the prerequisites, build the software, or learn more about the software architecture, see the  Software Setup Guide .  Hardware  To learn more about the hardware materials we used, and how we build our setup, check out the  Hardware Setup Guide .",
            "title": "Getting Started"
        },
        {
            "location": "/#video-walkthroughs",
            "text": "Calibrating CribSense   CribSense Demonstration",
            "title": "Video Walkthroughs"
        },
        {
            "location": "/setup/sw-setup/",
            "text": "Software Setup Guide\n\n\nThis is the step-by-step guide on building the cribsense software on Ubuntu.\n\n\nPrerequisites\n\n\nThis software depends on autoconf, OpenCV and libcanberra. Install these by running\n\n\nsudo apt-get install git build-essential libtool autoconf libopencv-dev libcanberra-dev\n\n\n\n\nNext you need to set to camera driver to autoload by adding \nbcm2835-v4l2\n to \n/etc/modules-load.d/modules.conf\n.\nYour file should look like this:\n\n\n# /etc/modules: kernel modules to load at boot time.\n#\n# This file contains the names of kernel modules that should be loaded\n# at boot time, one per line. Lines beginning with \"#\" are ignored.\n\ni2c-dev\nbcm2835-v4l2\n\n\n\n\nOnce that file has been edited, you must reboot your Pi.\n\n\nBuild\n\n\nTo build the software, navigate to the root of the repository directory and run\n\n\n./autogen.sh --prefix=/usr --sysconfdir=/etc --disable-debug\nmake\nsudo make install\nsudo systemctl daemon-reload\n\n\n\n\nUsage\n\n\nTo start the program in the background:\n\n\nsudo systemctl start cribsense\n\n\n\n\nTo run it in the foreground:\n\n\ncribsense --config /etc/cribsense/config.ini\n\n\n\n\nTo start the program automatically at every boot:\n\n\nsudo systemctl enable cribsense\n\n\n\n\nTo stop cribsense from automatically running at boot:\n\n\nsudo systemctl disable cribsense\n\n\n\n\nNote that when \ncribsense\n is started using \nsystemctl\n, the config parameters are already sent and are stored in \n/etc/systemd/system/cribsense.service\n\n\nOnce you have installed the software and have it configured well for your setting, you will no longer need the keyboard/mouse/monitor. Now, you can set your CribSense to autorun at every boot as shown above.\n\n\nNow, all you'll need to do is plug in speakers, attach the CribSense to your previously tested spot on the side of your crib, and plug it in. The software will automatically run after a few moments, and will begin tracking motion. When you're done, well, we don't have an elegant way for you to turn off the Pi yet. But, you can just unplug the speakers and leave it running (and be sure to set the infant in the crib for a couple minutes before plugging them back in). Directly unplugging the Pi may corrupt your SD card.\n\n\nRemember that if the environment changes, you'll probably need to reconfigure. You'll want to disable the autorun using the command above, and go through the calibration steps on the \nConfiguration Page\n.\n\n\nIf something doesn't seem to be working, you can checkout our \nTroubleshooting Page\n\n\nTip: If everything is set up, you can \nmake a backup of your Raspberry Pi SD Card\n for easy restoration if something goes wrong.\n\n\nSoftware Configuration\n\n\nCribSense customizable through a simple INI configuration file. After running \nmake install\n, the configuration file is located at:\n\n\nsudo nano /etc/cribsense/config.ini\n\n\n\n\nand it will look like this\n\n\n[io]                  ; I/O configuration\n; input = vid/noir_cam/lowres_10fps_orange_2min.h264   ; Input file to use\ninput_fps = 15          ; fps of input (40 max, 15 recommended if using camera)\nfull_fps = 4.5          ; fps at which full frames can be processed\ncrop_fps = 15           ; fps at which cropped frames can be processed\ncamera = 0              ; Camera to use\nwidth = 640             ; Width of the input video\nheight = 480            ; Height of the input video\ntime_to_alarm = 10      ; How many seconds to wait with no motion before alarm.\n\n[cropping]            ; Adaptive Cropping Settings\ncrop = true                 ; Whether or not to crop\nframes_to_settle = 10       ; # frames to wait after reset before processing\nroi_update_interval = 800   ; # frame between recalculating ROI\nroi_window = 50             ; # frames to monitor before selecting ROI\n\n[motion]              ; Motion Detection Settings\nerode_dim = 4           ; dimension of the erode kernel\ndilate_dim = 60         ; dimention of the dilate kernel\ndiff_threshold = 8      ; abs difference needed before recognizing change\nduration = 1            ; # frames to maintain motion before flagging true\npixel_threshold = 5     ; # pixels that must be different to flag as motion\nshow_diff = false       ; display the diff between 3 frames\n\n[magnification]       ; Video Magnification Settings\namplify = 25                ; The % amplification desired\nlow-cutoff = 0.5            ; The low frequency of the bandpass.\nhigh-cutoff = 1.0           ; The high frequency of the bandpass.\nthreshold = 50              ; The phase threshold as % of pi.\nshow_magnification = false  ; Show the output frames of each magnification\n\n[debug]\nprint_times = false ; Print analysis times\n\n\n\n\n\nView the full configuration details on the \nConfiguration Page\n\n\nSoftware Architecture Details\n\n\nThe CribSense is the heart and soul of this project. The rest is mainly a fun opportunity to use a 3D printer and do some soldering. We saw some of the great demos of video magnification from MIT, and wanted to try and run a similar algorithm on a Raspberry Pi. This required about 10x speedup from the great work of tbl3rd on his C++ implementation of video magnification in order to run in real-time on the Pi. The optimizations needed guided our design of the software.\n\n\nThe software for processing a video stream is implemented as a state machine that cycles as shown below. There are currently six states that manage all of our processing steps. Our state machine logic is called each time a new frame is read from the camera.\n\n\n\n\nInitialization\n\n\nWhen the video stream first turns on, it is common to have a flash of white or black pixels that looks like a lot of motion is happening. This state simply initializes the video magnification and motion detection code, and skips a few of the first frames before jumping into monitoring motion.\n\n\nMonitoring Motion\n\n\nIn this state, the full 640 x 480 frame is magnified, and uses an image differential algorithm from Collins et al. to calculate the motion pixels between frames. The output of this algorithm is a black and white image where white pixels indicate pixels that have changed. These black and white images are then bitwise ORed together for several frames to accumulate the motion seen over the period of time.\n\n\nComputing a Region of Interest\n\n\nWith the accumulated black and white frame representing the motion seen over several frames, the image is eroded slightly to eliminate noise, then significantly dilated to highlight the areas with the most motion. Dilation is necessary to merge discrete points into continuous regions of motion. Because of the large dilation, it is then easy to find the largest contour in the black and white image, which represents the main source of motion, and draw a bounding box around the contour. It is in this portion of code that we can set bounds on the size of the crop.\n\n\nSteady State\n\n\nThis is where our software spends most of its time. In this state, the software is operating on cropped frames. Video magnification is performed and a measure of the movement seen in the frame is calculated. In this state, we are able to process at a fast enough rate to catch up with any delay caused during the processing of full frames, and keep up with our 10fps video stream.\n\n\nPeriodic Reset\n\n\nBecause the baby may move occasionally, we periodically reset the video magnification and cropping so that we have full-resolution frames available to repeat the process of monitoring motion and finding a new region of interest. All of these timing parameters are easily configurable.\n\n\nTechnical Challenges\n\n\nSpeed\n\n\nWhen we started this project, our C++ implementation could magnify video at a rate of 394,000 pixels per second on the Raspberry Pi 3. For reference, this meant our code could only process a 640 x 480 video stream at a rate of approximately 1.3 frames per second.  Optimizing our code to handle the real-time goal was our primary objective and challenge, since we needed a 10x speed-up to handle at least 10 frames per second. Now, we are able to process over 1,200,000 pixels per second, and use additional cropping to process a 10 fps video stream in real time.\nWe used three main optimizations increase performance by 10x: (1) multithreading, (2) adaptive cropping, and (3) compiler optimization. Each optimization is explained in more detail below.\n\n\nMultithreading (~3x)\n\n\nOne intuitive and high-value optimization was to process sections of each video frame in parallel. To do so, we divide each frame vertically, such that each section's height is one-third of the original frame. Then, each section is magnified as if it was its own video stream. The resulting magnified frames are then joined together to form a full-resolution frame before evaluating the frame for motion. This optimization alone brought our processing rate to 938,000 pixels per second (roughly a 3x improvement). Three of the Raspberry Pi's four cores are dedicated to video magnification, while the 4th core handles control flow and motion processing.\n\n\nAdaptive Cropping (~3x)\n\n\nWhile multithreading sought to increase the number of pixels we could process each second, adaptive cropping sought to decrease the number of pixels we needed to process. We rely on two main assumptions for this optimization. First, the camera captures the entire crib in its field of view and the baby is the only source of movement in the frame.  Second, the camera is placed such that movement is discernible, and the region of interest is not larger than one-third of the frame. With these assumptions, we are able to periodically monitor a full frame to determine where the motion is occurring, then crop down the stream to only contain the relevant motion. By reducing the number of pixels that we process in our steady state, and amortizing the cost of monitoring a few full-resolution frames, we are able to increase the number of frames we can handle each second by about 3x.\n\n\nCompiler Optimizations (~1.5x)\n\n\nFinally, in order to reach our goal of over 10x speedup, we were able to make some code optimizations and utilize additional compiler flags. In particular, we make use of the -O3 optimization level, and we force the compiler to generate vector instructions with -ftree-vectorize and -mfpu=crypto-neon-fp-armv8. Additionally, we were required to add the -funsafe-math-optimization flag to enable the vector float generation, because NEON instructions are not IEEE compliant and gcc would not generate them otherwise.\nWe found two major challenges in enabling compiler optimization. The first was to expose the operations to the compiler. The most expensive part of transformation happens in two all-pixel loops, but previously each operation in the loop was implemented with an opencv call that would run the operation over all pixels. We rewrote the code to pull out the loop and avoid intermediate copies. On an x86-64 machine, this is already an improvement, because of larger caches and the fact that the compiler always issues at least SSE2 vector instructions. In contrast, the switch from vector instruction enabled opencv to straight C++ code resulted in worse performance on the Pi.\nWe recovered this performance drop by adding the compiler flags, but we encountered toolchain problems. The Pi 3 has an ARM Cortex A53 processor, which is an A-class 64-bit ARMv8 with NEON and crypto extensions, but it runs a 32-bit OS compiled for ARMv7. We found first that compiling with -march=native exposes a known but yet unfixed GCC bug. Compiling with -march=armv8 appears to work, but generates ABI incompatible binaries when linked with an ARMv7 libstdc++, leading to stack corruption. On the other hand, leaving the default of -march=armv7 and forcing just the FPU to generate NEON appears to work, looking at the assembly. We also added -mtune=cortex-a53 to achieve better instruction scheduling and better counts for loop unrolling (which is enabled at -O3), but we suspect it has no effect because the version of GCC we use (4.9.2, provided by Raspbian) does not recognize it.\n\n\nProfiling\n\n\nIn order to isolate further optimization spots, we employed profiling to identify where the code is spending most of the time.\nIn doing so, we also encountered toolchain problems, as valgrind is unable to emulate the NEON vector instructions and crashes. We therefore turned to gprof, but we found its timing somewhat unreliable, despite the -fno-inline switch (function call counts on the other hand are precise but not quite useful). Finally, we tried perf, which uses the HW performance counters and gives us instruction level timings, but we found that it is very precise inside the transformation code, and has zero data outside, because it sees no sample of the other code. Overall, profiling suggests that 96% of the time is spent in the transformation and 2% in motion detection, with the rest being threading and locking overhead, and accessing the camera.\n\n\nTiming\n\n\nOverall, our algorithm is able to process a full 640x480 frame in 220 to 240 ms, while after cropping our algorithm runs in the worst case (where the crop is approximately 320x320) at about 70 ms per frame. This allows us to maintain a steady 10 fps goal.",
            "title": "Software Setup Instructions"
        },
        {
            "location": "/setup/sw-setup/#software-setup-guide",
            "text": "This is the step-by-step guide on building the cribsense software on Ubuntu.",
            "title": "Software Setup Guide"
        },
        {
            "location": "/setup/sw-setup/#prerequisites",
            "text": "This software depends on autoconf, OpenCV and libcanberra. Install these by running  sudo apt-get install git build-essential libtool autoconf libopencv-dev libcanberra-dev  Next you need to set to camera driver to autoload by adding  bcm2835-v4l2  to  /etc/modules-load.d/modules.conf .\nYour file should look like this:  # /etc/modules: kernel modules to load at boot time.\n#\n# This file contains the names of kernel modules that should be loaded\n# at boot time, one per line. Lines beginning with \"#\" are ignored.\n\ni2c-dev\nbcm2835-v4l2  Once that file has been edited, you must reboot your Pi.",
            "title": "Prerequisites"
        },
        {
            "location": "/setup/sw-setup/#build",
            "text": "To build the software, navigate to the root of the repository directory and run  ./autogen.sh --prefix=/usr --sysconfdir=/etc --disable-debug\nmake\nsudo make install\nsudo systemctl daemon-reload",
            "title": "Build"
        },
        {
            "location": "/setup/sw-setup/#usage",
            "text": "To start the program in the background:  sudo systemctl start cribsense  To run it in the foreground:  cribsense --config /etc/cribsense/config.ini  To start the program automatically at every boot:  sudo systemctl enable cribsense  To stop cribsense from automatically running at boot:  sudo systemctl disable cribsense  Note that when  cribsense  is started using  systemctl , the config parameters are already sent and are stored in  /etc/systemd/system/cribsense.service  Once you have installed the software and have it configured well for your setting, you will no longer need the keyboard/mouse/monitor. Now, you can set your CribSense to autorun at every boot as shown above.  Now, all you'll need to do is plug in speakers, attach the CribSense to your previously tested spot on the side of your crib, and plug it in. The software will automatically run after a few moments, and will begin tracking motion. When you're done, well, we don't have an elegant way for you to turn off the Pi yet. But, you can just unplug the speakers and leave it running (and be sure to set the infant in the crib for a couple minutes before plugging them back in). Directly unplugging the Pi may corrupt your SD card.  Remember that if the environment changes, you'll probably need to reconfigure. You'll want to disable the autorun using the command above, and go through the calibration steps on the  Configuration Page .  If something doesn't seem to be working, you can checkout our  Troubleshooting Page  Tip: If everything is set up, you can  make a backup of your Raspberry Pi SD Card  for easy restoration if something goes wrong.",
            "title": "Usage"
        },
        {
            "location": "/setup/sw-setup/#software-configuration",
            "text": "CribSense customizable through a simple INI configuration file. After running  make install , the configuration file is located at:  sudo nano /etc/cribsense/config.ini  and it will look like this  [io]                  ; I/O configuration\n; input = vid/noir_cam/lowres_10fps_orange_2min.h264   ; Input file to use\ninput_fps = 15          ; fps of input (40 max, 15 recommended if using camera)\nfull_fps = 4.5          ; fps at which full frames can be processed\ncrop_fps = 15           ; fps at which cropped frames can be processed\ncamera = 0              ; Camera to use\nwidth = 640             ; Width of the input video\nheight = 480            ; Height of the input video\ntime_to_alarm = 10      ; How many seconds to wait with no motion before alarm.\n\n[cropping]            ; Adaptive Cropping Settings\ncrop = true                 ; Whether or not to crop\nframes_to_settle = 10       ; # frames to wait after reset before processing\nroi_update_interval = 800   ; # frame between recalculating ROI\nroi_window = 50             ; # frames to monitor before selecting ROI\n\n[motion]              ; Motion Detection Settings\nerode_dim = 4           ; dimension of the erode kernel\ndilate_dim = 60         ; dimention of the dilate kernel\ndiff_threshold = 8      ; abs difference needed before recognizing change\nduration = 1            ; # frames to maintain motion before flagging true\npixel_threshold = 5     ; # pixels that must be different to flag as motion\nshow_diff = false       ; display the diff between 3 frames\n\n[magnification]       ; Video Magnification Settings\namplify = 25                ; The % amplification desired\nlow-cutoff = 0.5            ; The low frequency of the bandpass.\nhigh-cutoff = 1.0           ; The high frequency of the bandpass.\nthreshold = 50              ; The phase threshold as % of pi.\nshow_magnification = false  ; Show the output frames of each magnification\n\n[debug]\nprint_times = false ; Print analysis times  View the full configuration details on the  Configuration Page",
            "title": "Software Configuration"
        },
        {
            "location": "/setup/sw-setup/#software-architecture-details",
            "text": "The CribSense is the heart and soul of this project. The rest is mainly a fun opportunity to use a 3D printer and do some soldering. We saw some of the great demos of video magnification from MIT, and wanted to try and run a similar algorithm on a Raspberry Pi. This required about 10x speedup from the great work of tbl3rd on his C++ implementation of video magnification in order to run in real-time on the Pi. The optimizations needed guided our design of the software.  The software for processing a video stream is implemented as a state machine that cycles as shown below. There are currently six states that manage all of our processing steps. Our state machine logic is called each time a new frame is read from the camera.   Initialization  When the video stream first turns on, it is common to have a flash of white or black pixels that looks like a lot of motion is happening. This state simply initializes the video magnification and motion detection code, and skips a few of the first frames before jumping into monitoring motion.  Monitoring Motion  In this state, the full 640 x 480 frame is magnified, and uses an image differential algorithm from Collins et al. to calculate the motion pixels between frames. The output of this algorithm is a black and white image where white pixels indicate pixels that have changed. These black and white images are then bitwise ORed together for several frames to accumulate the motion seen over the period of time.  Computing a Region of Interest  With the accumulated black and white frame representing the motion seen over several frames, the image is eroded slightly to eliminate noise, then significantly dilated to highlight the areas with the most motion. Dilation is necessary to merge discrete points into continuous regions of motion. Because of the large dilation, it is then easy to find the largest contour in the black and white image, which represents the main source of motion, and draw a bounding box around the contour. It is in this portion of code that we can set bounds on the size of the crop.  Steady State  This is where our software spends most of its time. In this state, the software is operating on cropped frames. Video magnification is performed and a measure of the movement seen in the frame is calculated. In this state, we are able to process at a fast enough rate to catch up with any delay caused during the processing of full frames, and keep up with our 10fps video stream.  Periodic Reset  Because the baby may move occasionally, we periodically reset the video magnification and cropping so that we have full-resolution frames available to repeat the process of monitoring motion and finding a new region of interest. All of these timing parameters are easily configurable.",
            "title": "Software Architecture Details"
        },
        {
            "location": "/setup/sw-setup/#technical-challenges",
            "text": "Speed  When we started this project, our C++ implementation could magnify video at a rate of 394,000 pixels per second on the Raspberry Pi 3. For reference, this meant our code could only process a 640 x 480 video stream at a rate of approximately 1.3 frames per second.  Optimizing our code to handle the real-time goal was our primary objective and challenge, since we needed a 10x speed-up to handle at least 10 frames per second. Now, we are able to process over 1,200,000 pixels per second, and use additional cropping to process a 10 fps video stream in real time.\nWe used three main optimizations increase performance by 10x: (1) multithreading, (2) adaptive cropping, and (3) compiler optimization. Each optimization is explained in more detail below.  Multithreading (~3x)  One intuitive and high-value optimization was to process sections of each video frame in parallel. To do so, we divide each frame vertically, such that each section's height is one-third of the original frame. Then, each section is magnified as if it was its own video stream. The resulting magnified frames are then joined together to form a full-resolution frame before evaluating the frame for motion. This optimization alone brought our processing rate to 938,000 pixels per second (roughly a 3x improvement). Three of the Raspberry Pi's four cores are dedicated to video magnification, while the 4th core handles control flow and motion processing.  Adaptive Cropping (~3x)  While multithreading sought to increase the number of pixels we could process each second, adaptive cropping sought to decrease the number of pixels we needed to process. We rely on two main assumptions for this optimization. First, the camera captures the entire crib in its field of view and the baby is the only source of movement in the frame.  Second, the camera is placed such that movement is discernible, and the region of interest is not larger than one-third of the frame. With these assumptions, we are able to periodically monitor a full frame to determine where the motion is occurring, then crop down the stream to only contain the relevant motion. By reducing the number of pixels that we process in our steady state, and amortizing the cost of monitoring a few full-resolution frames, we are able to increase the number of frames we can handle each second by about 3x.  Compiler Optimizations (~1.5x)  Finally, in order to reach our goal of over 10x speedup, we were able to make some code optimizations and utilize additional compiler flags. In particular, we make use of the -O3 optimization level, and we force the compiler to generate vector instructions with -ftree-vectorize and -mfpu=crypto-neon-fp-armv8. Additionally, we were required to add the -funsafe-math-optimization flag to enable the vector float generation, because NEON instructions are not IEEE compliant and gcc would not generate them otherwise.\nWe found two major challenges in enabling compiler optimization. The first was to expose the operations to the compiler. The most expensive part of transformation happens in two all-pixel loops, but previously each operation in the loop was implemented with an opencv call that would run the operation over all pixels. We rewrote the code to pull out the loop and avoid intermediate copies. On an x86-64 machine, this is already an improvement, because of larger caches and the fact that the compiler always issues at least SSE2 vector instructions. In contrast, the switch from vector instruction enabled opencv to straight C++ code resulted in worse performance on the Pi.\nWe recovered this performance drop by adding the compiler flags, but we encountered toolchain problems. The Pi 3 has an ARM Cortex A53 processor, which is an A-class 64-bit ARMv8 with NEON and crypto extensions, but it runs a 32-bit OS compiled for ARMv7. We found first that compiling with -march=native exposes a known but yet unfixed GCC bug. Compiling with -march=armv8 appears to work, but generates ABI incompatible binaries when linked with an ARMv7 libstdc++, leading to stack corruption. On the other hand, leaving the default of -march=armv7 and forcing just the FPU to generate NEON appears to work, looking at the assembly. We also added -mtune=cortex-a53 to achieve better instruction scheduling and better counts for loop unrolling (which is enabled at -O3), but we suspect it has no effect because the version of GCC we use (4.9.2, provided by Raspbian) does not recognize it.  Profiling  In order to isolate further optimization spots, we employed profiling to identify where the code is spending most of the time.\nIn doing so, we also encountered toolchain problems, as valgrind is unable to emulate the NEON vector instructions and crashes. We therefore turned to gprof, but we found its timing somewhat unreliable, despite the -fno-inline switch (function call counts on the other hand are precise but not quite useful). Finally, we tried perf, which uses the HW performance counters and gives us instruction level timings, but we found that it is very precise inside the transformation code, and has zero data outside, because it sees no sample of the other code. Overall, profiling suggests that 96% of the time is spent in the transformation and 2% in motion detection, with the rest being threading and locking overhead, and accessing the camera.  Timing  Overall, our algorithm is able to process a full 640x480 frame in 220 to 240 ms, while after cropping our algorithm runs in the worst case (where the crop is approximately 320x320) at about 70 ms per frame. This allows us to maintain a steady 10 fps goal.",
            "title": "Technical Challenges"
        },
        {
            "location": "/setup/hw-setup/",
            "text": "Hardare Setup Guide\n\n\n\n\nFigure: Hardware Block Diagram\n\n\nCribSense is relatively easy to construct, and is largely made up of commercially available parts.\nAs seen in the figure above, there are 5 main hardware components, only 2 of which are custom made.\nThis page contains the build instructions for CribSense.\n\n\nWhat you'll need\n\n\nRaspberry Pi + Camera + configuration tools:\n\n\n\n\nRaspberry Pi 3 Model B\n\n\n5V 2.5A Micro USB Power Supply\n\n\nRaspberry Pi NoIR Camera Module V2\n\n\n1W IR LED\n\n\nMicroSD Card\n (we used a Class 10 16GB card, the faster the card the better)\n\n\nFlex Cable for Raspberry Pi Camera (12\")\n\n\nSpeakers with 3.5mm input\n\n\nHDMI monitor\n\n\nUSB Keyboard\n\n\nUSB Mouse\n\n\n[optional] \nRaspberry Pi Heatsink\n (If you're worried about heat, you can stick these onto your Pi.)\n\n\n\n\nIR LED Circuit for low-light operation:\n\n\n\n\n[3x] 1N4001 diodes\n\n\n1 ohm, 1W resistor\n\n\n[2x] 12\" Wires with pin headers\n\n\nSoldering iron\n\n\n\n\nChassis:\n\n\n\n\nAccess to a 3D printer (minimum build volume = 9.9\" L x 7.8\" W x 5.9\" H) to print the chassis\n\n\nGlue (any type of glue will will work, but hot glue is recommended for prototyping)\n\n\n\n\nPrerequisites\n\n\nBefore you start our step-by-step guide, you should have already installed the latest version of \nRaspbian\n on your SD card and ensured that your Pi is functional and booting.\nYou'll also need to \nenable the camera module\n before you'll be able to interface with the camera.\n\n\nBuild Instructions\n\n\nSwap NoIR Camera Cable\n\n\nThe 6\" cable that comes with the camera is too short.\nSwap the short one (6\") with the longer one (12\").\nTo do this, you can follow \nthis guide from ModMyPi\n.\nTo summarize, there is a push/pull tab on the back of the NoIR camera, just like the one found on the Pi itself:\n\n\n\n\nSimply pull the black plastic tab out, remove the short cable, replace it with the long cable (making sure that the blue plastic strip is facing up still as shown in the picture), and push the tab back in to secure it.\n\n\n3D Printed Chassis\n\n\n\n  \n\n\n\n\nUsing our chassis is optional, though recommended to prevent young children from touching exposed electronic circuitry.\nEvery crib is different, so our chassis does not include include a mounting bracket.\nSeveral mounting options could include:\n\n\n\n\nCable Ties\n\n\n3M Dual Lock\n\n\nVelcro\n\n\nTape\n\n\n\n\nIf you have access to a MakerBot Replicator (5th Generation), you can simply download the \n.makerbot\n files for the \ncase\n and \ncover\n onto your MakerBot Replicator and print.\nIt takes about 6 hours to print the case and 3 hours to print the cover.\nIf you are using a different type of 3D printer, please keep reading.\n\n\nAs mentioned above, a minimum build volume of 9.9\" (L) x 7.8\" (W) x 5.9\" (H) is required to print CribSense.\nIf you do not have access to a 3D printer with this build volume, you can use an online 3D printing service (such as \nShapeways\n or \nSculpteo\n) to print CribSense.\nThe minimum print resolution is 0.015\".\nIf you are using a \nfused filament fabrication\n type 3D printer, this means that your nozzle diameter needs to be 0.015\" or smaller.\nPrinters with lower print resolutions (larger nozzle diameters) may work, but the Raspberry Pi might not fit into the chassis.\nWe recommend PLA (polylactic acid) as the preferred printing material.\nOther plastics may work, but the Raspberry Pi may not fit in the case if the thermal expansion coefficient of the chosen plastic is larger than that of PLA.\nIf your 3D printer has a heated build plate, turn off the heater before proceeding.\n\n\nOrienting the model on your printer's build plate is critical for a successful print.\nThese models were carefully designed so they do not need to be printed with support material, thus saving plastic and improving print quality.\nBefore proceeding, download the 3D files for the \ncase\n and \ncover\n.\nWhen printing these models, the neck of CribSense must lay flat on the build plate.\nThis ensures that all overhang angles on the models do not exceed 45 degrees, thus eliminating the requirement for support material.\nFor instructions on orientating 3D models in the build volume of your printer, please refer to the instruction manual that comes with your 3D printer.\nExamples for the build orientation of the case and cover are shown below.\n\n\nCase:\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nCover:\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nIn addition to putting the neck of CribSense flat against the build plate, you may notice that the models are rotated around the vertical axis.\nThis may be necessary to fit the model inside the build volume of your 3D printer.\nThis rotation is optional if the length of your build volume is long enough to accommodate CribSense.\n\n\nIR LED Circuit\n\n\nIn order to provide adequate lighting at night, we use an IR LED, which is not visible to the human eye but visible to the NoIR camera.\nThe IR LED does not consume a lot of power compared to the Raspberry Pi, so we leave the IR LED on for the sake of simplicity.\n\n\nTo power the LED from the GPIO header pins on the Pi, we construct the circuit in the figure below.\n\n\n\n\nFigure: LED Schematic\n\n\nIn earlier versions of the Pi, the maximum output current of these pins was \n50mA\n.\nThe Raspberry Pi B+ increased this to 500mA.\nHowever, for simplicity and backwards compatibility, we use the 5V power pins, which can supply up to \n1.5A\n.\nThe forward voltage of the IR LED is about 1.7~1.9V according to our measurements.\nEven though the IR LED has a maximum current of 500mA, we decided to reduce the current to around 200mA to reduce heat and overall power consumption.\nExperimental results also show that the IR LED is bright enough with 200 mA of input current.\nTo bridge the gap between 5V and 1.9V, we decided to use three 1N4001 diodes and a 1 Ohm resistor in series with the IR LED.\nThe voltage drop over the wire, diodes and resistor is about 0.2V, 0.9V (for each one) and 0.2V, respectively.\nThus, the voltage over the IR LED is \n5V - 0.2V - (3 * 0.9V) - 0.2V = 1.9V\n.\nThe heat dissipation over each LED is 0.18W, and is 0.2W over the resistor, all well within the maximum ratings.\n\n\nThe circuit should looking something like this:\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nBut, we're not done yet!\nIn order to get a better fit in the 3D printed chassis, we want to have the IR LED lens protrude from our chassis and have the PCB board flush with the hole.\nThe small photodiode in the bottom right will get in the way.\nTo remedy this, we desolder it and flip it to the opposite side of the board like this:\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\nThe photodiode is not needed since we want the LED to always be on.\nSimply switching it to the opposite side leaves the original LED circuit unchanged.\n\n\nAssembly: Bring it all together\n\n\nOnce you have all the hardware ready, you can begin assembly.\nAny glue can be used in this process, but we recommend hot glue for two main reasons.\nHot glue drys quickly, so you do not need to wait a long time for the glue to dry.\nIn addition, hot glue is removable if you make a mistake.\nTo remove dried hot glue, soak the hot glue in in rubbing (isopropyl) alcohol.\nWe recommend 90% concentration or higher, but 70% concentration will still work.\nSoaking the dried hot glue in isopropyl alcohol will weaken the bond between the glue and underlying surface, allowing you to peel the glue off cleanly.\nWhen soaking the glue in isopropyl alcohol, the Raspberry Pi should be powered off and unplugged.\nBe sure to let everything dry before reapplying hot glue and booting the Raspberry Pi.\n\n\nThe assembly instructions below assume that you are using hot glue.\nIf you are using a different type of glue, the assembly instructions still apply, though the drying times may differ.\nThroughout this process, make sure that the glue is dry before moving on to the next step.\nThroughout the build, check to make sure that all the ports can still be accessed through the holes in the CribSense chassis.\nClick any of the pictures below for higher resolution images.\n\n\n\n\nInsert the Raspberry Pi into the chassis. Once it is in place, be sure that all of the ports can still be accessed (e.g. you can plug in power power).\n\n\n\n\n\n  \n\n\n\n\n\n\nNext, use hot glue to tack the Pi into place and attach the camera to the Pi. There are screw holes as well if you prefer to use those.\n\n\n\n\n\n  \n\n\n\n\n\n\nGlue the LED and camera to the front cover.\n\n\n\n\n\n  \n\n\n\n\n\n\nFirst, glue the NoIR camera to the camera hole. Be sure that the camera is snug and lined up with the chassis. Do not use too much glue; otherwise, you will not be able to fit the camera into the main case. Be sure to power on the Pi and take a look at the camera (\nraspistill -v\n, for example) to make sure that it is angled well and has a good field of view. If it is not, remove the hot glue and reposition it.\n\n\n\n\n\n  \n\n\n\n\n\n\nNext, glue the IR LED to the hole on the neck of the cover. It's at a 45 degree angle to side light the crib, which results in more shadows in low-light situations. This adds more contrast to the image, making it easier to detect motion.\n\n\n\n\n\n  \n\n\n\n\n\n\nWith both of them glued to the neck, it should look like this:\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n\nAttach the IR LED wires to the Raspberry Pi's GPIO pins as shown in the LED Schematic figure.\n\n\nPack the cables into the chassis in a way that does not crease or strain them. We ended up folding the cable accordion style because our camera flex cable was too long.\n\n\n\n\n\n  \n\n\n\n\n\n\nWith everything tucked in, hot glue around the edges where the two pieces meet, sealing them in place.",
            "title": "Hardware Setup Instructions"
        },
        {
            "location": "/setup/hw-setup/#hardare-setup-guide",
            "text": "Figure: Hardware Block Diagram  CribSense is relatively easy to construct, and is largely made up of commercially available parts.\nAs seen in the figure above, there are 5 main hardware components, only 2 of which are custom made.\nThis page contains the build instructions for CribSense.",
            "title": "Hardare Setup Guide"
        },
        {
            "location": "/setup/hw-setup/#what-youll-need",
            "text": "Raspberry Pi + Camera + configuration tools:   Raspberry Pi 3 Model B  5V 2.5A Micro USB Power Supply  Raspberry Pi NoIR Camera Module V2  1W IR LED  MicroSD Card  (we used a Class 10 16GB card, the faster the card the better)  Flex Cable for Raspberry Pi Camera (12\")  Speakers with 3.5mm input  HDMI monitor  USB Keyboard  USB Mouse  [optional]  Raspberry Pi Heatsink  (If you're worried about heat, you can stick these onto your Pi.)   IR LED Circuit for low-light operation:   [3x] 1N4001 diodes  1 ohm, 1W resistor  [2x] 12\" Wires with pin headers  Soldering iron   Chassis:   Access to a 3D printer (minimum build volume = 9.9\" L x 7.8\" W x 5.9\" H) to print the chassis  Glue (any type of glue will will work, but hot glue is recommended for prototyping)",
            "title": "What you'll need"
        },
        {
            "location": "/setup/hw-setup/#prerequisites",
            "text": "Before you start our step-by-step guide, you should have already installed the latest version of  Raspbian  on your SD card and ensured that your Pi is functional and booting.\nYou'll also need to  enable the camera module  before you'll be able to interface with the camera.",
            "title": "Prerequisites"
        },
        {
            "location": "/setup/hw-setup/#build-instructions",
            "text": "Swap NoIR Camera Cable  The 6\" cable that comes with the camera is too short.\nSwap the short one (6\") with the longer one (12\").\nTo do this, you can follow  this guide from ModMyPi .\nTo summarize, there is a push/pull tab on the back of the NoIR camera, just like the one found on the Pi itself:   Simply pull the black plastic tab out, remove the short cable, replace it with the long cable (making sure that the blue plastic strip is facing up still as shown in the picture), and push the tab back in to secure it.  3D Printed Chassis  \n     Using our chassis is optional, though recommended to prevent young children from touching exposed electronic circuitry.\nEvery crib is different, so our chassis does not include include a mounting bracket.\nSeveral mounting options could include:   Cable Ties  3M Dual Lock  Velcro  Tape   If you have access to a MakerBot Replicator (5th Generation), you can simply download the  .makerbot  files for the  case  and  cover  onto your MakerBot Replicator and print.\nIt takes about 6 hours to print the case and 3 hours to print the cover.\nIf you are using a different type of 3D printer, please keep reading.  As mentioned above, a minimum build volume of 9.9\" (L) x 7.8\" (W) x 5.9\" (H) is required to print CribSense.\nIf you do not have access to a 3D printer with this build volume, you can use an online 3D printing service (such as  Shapeways  or  Sculpteo ) to print CribSense.\nThe minimum print resolution is 0.015\".\nIf you are using a  fused filament fabrication  type 3D printer, this means that your nozzle diameter needs to be 0.015\" or smaller.\nPrinters with lower print resolutions (larger nozzle diameters) may work, but the Raspberry Pi might not fit into the chassis.\nWe recommend PLA (polylactic acid) as the preferred printing material.\nOther plastics may work, but the Raspberry Pi may not fit in the case if the thermal expansion coefficient of the chosen plastic is larger than that of PLA.\nIf your 3D printer has a heated build plate, turn off the heater before proceeding.  Orienting the model on your printer's build plate is critical for a successful print.\nThese models were carefully designed so they do not need to be printed with support material, thus saving plastic and improving print quality.\nBefore proceeding, download the 3D files for the  case  and  cover .\nWhen printing these models, the neck of CribSense must lay flat on the build plate.\nThis ensures that all overhang angles on the models do not exceed 45 degrees, thus eliminating the requirement for support material.\nFor instructions on orientating 3D models in the build volume of your printer, please refer to the instruction manual that comes with your 3D printer.\nExamples for the build orientation of the case and cover are shown below.  Case:  \n     \n     Cover:  \n     \n     In addition to putting the neck of CribSense flat against the build plate, you may notice that the models are rotated around the vertical axis.\nThis may be necessary to fit the model inside the build volume of your 3D printer.\nThis rotation is optional if the length of your build volume is long enough to accommodate CribSense.  IR LED Circuit  In order to provide adequate lighting at night, we use an IR LED, which is not visible to the human eye but visible to the NoIR camera.\nThe IR LED does not consume a lot of power compared to the Raspberry Pi, so we leave the IR LED on for the sake of simplicity.  To power the LED from the GPIO header pins on the Pi, we construct the circuit in the figure below.   Figure: LED Schematic  In earlier versions of the Pi, the maximum output current of these pins was  50mA .\nThe Raspberry Pi B+ increased this to 500mA.\nHowever, for simplicity and backwards compatibility, we use the 5V power pins, which can supply up to  1.5A .\nThe forward voltage of the IR LED is about 1.7~1.9V according to our measurements.\nEven though the IR LED has a maximum current of 500mA, we decided to reduce the current to around 200mA to reduce heat and overall power consumption.\nExperimental results also show that the IR LED is bright enough with 200 mA of input current.\nTo bridge the gap between 5V and 1.9V, we decided to use three 1N4001 diodes and a 1 Ohm resistor in series with the IR LED.\nThe voltage drop over the wire, diodes and resistor is about 0.2V, 0.9V (for each one) and 0.2V, respectively.\nThus, the voltage over the IR LED is  5V - 0.2V - (3 * 0.9V) - 0.2V = 1.9V .\nThe heat dissipation over each LED is 0.18W, and is 0.2W over the resistor, all well within the maximum ratings.  The circuit should looking something like this:  \n     \n     But, we're not done yet!\nIn order to get a better fit in the 3D printed chassis, we want to have the IR LED lens protrude from our chassis and have the PCB board flush with the hole.\nThe small photodiode in the bottom right will get in the way.\nTo remedy this, we desolder it and flip it to the opposite side of the board like this:  \n     \n     The photodiode is not needed since we want the LED to always be on.\nSimply switching it to the opposite side leaves the original LED circuit unchanged.  Assembly: Bring it all together  Once you have all the hardware ready, you can begin assembly.\nAny glue can be used in this process, but we recommend hot glue for two main reasons.\nHot glue drys quickly, so you do not need to wait a long time for the glue to dry.\nIn addition, hot glue is removable if you make a mistake.\nTo remove dried hot glue, soak the hot glue in in rubbing (isopropyl) alcohol.\nWe recommend 90% concentration or higher, but 70% concentration will still work.\nSoaking the dried hot glue in isopropyl alcohol will weaken the bond between the glue and underlying surface, allowing you to peel the glue off cleanly.\nWhen soaking the glue in isopropyl alcohol, the Raspberry Pi should be powered off and unplugged.\nBe sure to let everything dry before reapplying hot glue and booting the Raspberry Pi.  The assembly instructions below assume that you are using hot glue.\nIf you are using a different type of glue, the assembly instructions still apply, though the drying times may differ.\nThroughout this process, make sure that the glue is dry before moving on to the next step.\nThroughout the build, check to make sure that all the ports can still be accessed through the holes in the CribSense chassis.\nClick any of the pictures below for higher resolution images.   Insert the Raspberry Pi into the chassis. Once it is in place, be sure that all of the ports can still be accessed (e.g. you can plug in power power).   \n      Next, use hot glue to tack the Pi into place and attach the camera to the Pi. There are screw holes as well if you prefer to use those.   \n      Glue the LED and camera to the front cover.   \n      First, glue the NoIR camera to the camera hole. Be sure that the camera is snug and lined up with the chassis. Do not use too much glue; otherwise, you will not be able to fit the camera into the main case. Be sure to power on the Pi and take a look at the camera ( raspistill -v , for example) to make sure that it is angled well and has a good field of view. If it is not, remove the hot glue and reposition it.   \n      Next, glue the IR LED to the hole on the neck of the cover. It's at a 45 degree angle to side light the crib, which results in more shadows in low-light situations. This adds more contrast to the image, making it easier to detect motion.   \n      With both of them glued to the neck, it should look like this:   \n     \n      Attach the IR LED wires to the Raspberry Pi's GPIO pins as shown in the LED Schematic figure.  Pack the cables into the chassis in a way that does not crease or strain them. We ended up folding the cable accordion style because our camera flex cable was too long.   \n      With everything tucked in, hot glue around the edges where the two pieces meet, sealing them in place.",
            "title": "Build Instructions"
        },
        {
            "location": "/setup/config/",
            "text": "Configuration\n\n\nConfiguration of CribSense uses a file which is usually located in \n/etc/cribsense/config.ini\n.\n\n\nThe format is that of INI files: each line is a configuration value, and directives are grouped in section introduced by square brackets. Comments are delimited with ; to the end of the line.\n\n\nInput/Output\n\n\nThe \n[io]\n section of the file configures the input to CribSense.\nYou should not need to modify this section, as it is already calibrated to work on the Raspberry Pi.\n\n\nThe \ninput\n directive specifies that the magnifier should look for a video file instead of real-time camera input, while \ncamera\n directive chooses the camera device to use (eg. \ncamera = 0\n means to use \n/dev/video0\n as input). File input exists as a demo or debugging feature only.\nIf you change the input, you must also specify the fps parameters to match the input. For file input, there is only one fps setting, because frames are never dropped, while for camera input, to reduce latency you must specify the frame per second at full frame size and at cropped frame size (roughly 3x the full frame size fps). The latter values depend on the speed of the CPU on which cribsense run.\n\n\ntime_to_alarm\n determines the number of seconds to wait after cribsense stops seeing motion before playing an alarm sound through the audio port.\n\n\nCropping\n\n\nThe \n[cropping]\n section controls the adaptive motion-based cropping, which focuses the magnification process on a smaller Region of Interest (ROI) where the most motion is occurring, reducing the CPU load.\n\n\nThe different parameters affect the latency of detection, as they control the number of \"slow\" frames (fully uncropped). Optimal values for the parameters depend on the target CPU. If you're running on a different device than the Pi, and it's sufficiently powerful, you can also disable cropping altogether.\n\n\nThe default configuration will update the crop approximately every minute.\n\n\nMotion & Magnification\n\n\nThe \n[motion]\n and \n[magnification]\n sections control the motion detection and video magnification algorithm respectively.\n\n\nThese parameters depend on the setting in which the cribsense is deployed, such as lighting condition and contrast on the baby.\n\n\nIn general, you should not need to change the magnification setting, as it is tuned to detect normal breathing rates.\n\n\nCalibration may be needed for \nerode_dim\n and \ndilate_dim\n, which are used to determine where to crop the video, as well as \ndiff_threshold\n and \npixel_threshold\n which determine the thresholds used judge the magnitude of change in pixels and the number of pixels of motion are needed before being classified as motion rather than noise.\n\n\nSpecifically, \ndiff_threshold\n specifies the amount of change a grayscale pixel (value between 0 and 255) will need to change before being marked as changed. For example, if \ndiff_threshold\n is 30, the difference between a pixels has to be equal to or more than 30 to be marked changed. The higher this threshold, the more the pixels needs to be different to be marked.\n\n\npixel_threshold\n defines a hard cut of in the number of pixels that need to be marked as changed before outputting motion values. This effectively sets the cutoff for noise when determining whether or not an infant is breathing. For example, if \npixel_threshold\n is set to 100, the algorithm must see more than 100 pixels of change before registering any motion as seen. It will report no motion if that threshold is not crossed.\n\n\nerode_dim\n specifies the dimension of the kernel to use in an \nOpenCV erode operation\n. This is used to minimize the changed pixels. That is, pixels that are isolated will be removed, but when large groups of pixels are changed, they will remain. \ndilate_dim\n is the opposite, it takes a pixels and expands it. These two parameters are used when detecting the region of the frame that the infant is located in. First, the pixel differences are calculated, then, a small erosion is applied to eliminate noise and a large dilation is applied to broadly mark the areas of motion.\n\n\nlow-cuttoff\n and \nhigh-cutoff\n define the range of the bandpass filter used during magnification. Specifically, video magnification will try to magnify motion that occurs within this frequency range, and ignore motion outside this range. We've tuned this to be able to capture breathing rats in general, but you may need to tweak this during calibration.\n\n\nSee the section on calibration for more information.\n\n\nDebugging features\n\n\nThe \nshow_diff\n flag in \n[motion]\n will show a window where the areas where motion is detected in the frame are highlighted in white. The \nshow_magnification\n flag in \n[magnification]\n controls a window that shows the output of just video magnification (which should look like the camera feed, in black and white, which enlarged motion).\nYou can use this two flags to show the result of your changes to the motion and magnification parameters.\n\n\nFinally, the \nprint_times\n in the \n[debug]\n section controls printing of frame times in the standard output, which you can use to calibrate the FPS and latency settings when running on a device different than the Raspberry Pi.\n\n\nThese features must be left to off when cribsense is started through systemd (automatically on boot or with \nsystemctl start\n). They are only useful if you run cribsense manually.\n\n\nCalibrating the Motion & Magnification algorithm\n\n\nCalibration of the algorithm is an iterative effort, with no right or wrong answer. We encourage you to experiment with various values, combining them with the debugging features, to find the combination of parameters most suitable to your environment.\n\n\nAs a guideline, increasing the \namplification\n and the \nphase_threshold\n values increases the amount of magnification applied to the input video. You should change these values until you clearly see the movement from your baby breathing, and no significant artifact in other areas of the video. If you experience artifacts, reducing the \nphase_threshold\n while keeping the same \namplification\n might help. You can view the effects of these parameters by setting \nshow_magnification\n to \ntrue\n.\n\n\nAs for the motion detection parameters, the main driver is the amount of noise. When detecting regions of motion to crop to, \nerode_dim\n and \ndilate_dim\n are used to size the dimensions of the OpenCV kernels used to \nerode and dilate\n motion so that noise is first eroded away, then the remaining motion signal is significantly dilated to make the regions of motion obvious. These parameters may also need to be tuned if your crib is in a very high-contrast setting. In general, you will need a higher \nerode_dim\n for high contrast settings, and a lower \nerode_dim\n for low contrast.\n\n\nIf you run with \nshow_diff = true\n and you notice that too much of the input video is white, or some completely unrelated part of the video is detected as motion (e.g. a flickering lamp), you'll want to increase the \nerode_dim\n until only the part of the video corresponding to your breathing baby is the largest section of white. The top figure shows an example where the erode dimension is too low for the amount of motion in the frame, while the bottom one shows a correctly calibrated frame.\n\n\n\n\n\n\nOnce this has been calibrated, you'll want to make sure that the \npixel_threshold\n is set so that motion is only reported when you expect, and not constantly (which means you need to cut out the noise). Ideally, you'll see output like this in your terminal:\n\n\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 44    [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 161   [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 121   [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 86    [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 97    [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 74    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 60    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 48    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 38    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 29    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 28    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 22    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n\n\n\n\nWhere there is a clear periodic pattern corresponding to the motion.\n\n\nIf your output looks more like this:\n\n\n[info] Pixel Movement: 921   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 736   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 666   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 663   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 1196  [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 1235  [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 1187  [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 1115  [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 959   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 744   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 611   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 468   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 371   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 307   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 270   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 234   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 197   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 179   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 164   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 239   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 733   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 686   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 667   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 607   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 544   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 499   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 434   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 396   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 375   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 389   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 305   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 269   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 1382  [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 1086  [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 1049  [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 811   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 601   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 456   [info] Motion Estimate: 1.312346 Hz\n\n\n\n\nThen you'll need to adjust \npixel_threshold\n and \ndiff_threshold\n until just peaks are seen, and pixel movement is 0 otherwise.",
            "title": "Software Configuration Parameters"
        },
        {
            "location": "/setup/config/#configuration",
            "text": "Configuration of CribSense uses a file which is usually located in  /etc/cribsense/config.ini .  The format is that of INI files: each line is a configuration value, and directives are grouped in section introduced by square brackets. Comments are delimited with ; to the end of the line.",
            "title": "Configuration"
        },
        {
            "location": "/setup/config/#inputoutput",
            "text": "The  [io]  section of the file configures the input to CribSense.\nYou should not need to modify this section, as it is already calibrated to work on the Raspberry Pi.  The  input  directive specifies that the magnifier should look for a video file instead of real-time camera input, while  camera  directive chooses the camera device to use (eg.  camera = 0  means to use  /dev/video0  as input). File input exists as a demo or debugging feature only.\nIf you change the input, you must also specify the fps parameters to match the input. For file input, there is only one fps setting, because frames are never dropped, while for camera input, to reduce latency you must specify the frame per second at full frame size and at cropped frame size (roughly 3x the full frame size fps). The latter values depend on the speed of the CPU on which cribsense run.  time_to_alarm  determines the number of seconds to wait after cribsense stops seeing motion before playing an alarm sound through the audio port.",
            "title": "Input/Output"
        },
        {
            "location": "/setup/config/#cropping",
            "text": "The  [cropping]  section controls the adaptive motion-based cropping, which focuses the magnification process on a smaller Region of Interest (ROI) where the most motion is occurring, reducing the CPU load.  The different parameters affect the latency of detection, as they control the number of \"slow\" frames (fully uncropped). Optimal values for the parameters depend on the target CPU. If you're running on a different device than the Pi, and it's sufficiently powerful, you can also disable cropping altogether.  The default configuration will update the crop approximately every minute.",
            "title": "Cropping"
        },
        {
            "location": "/setup/config/#motion-magnification",
            "text": "The  [motion]  and  [magnification]  sections control the motion detection and video magnification algorithm respectively.  These parameters depend on the setting in which the cribsense is deployed, such as lighting condition and contrast on the baby.  In general, you should not need to change the magnification setting, as it is tuned to detect normal breathing rates.  Calibration may be needed for  erode_dim  and  dilate_dim , which are used to determine where to crop the video, as well as  diff_threshold  and  pixel_threshold  which determine the thresholds used judge the magnitude of change in pixels and the number of pixels of motion are needed before being classified as motion rather than noise.  Specifically,  diff_threshold  specifies the amount of change a grayscale pixel (value between 0 and 255) will need to change before being marked as changed. For example, if  diff_threshold  is 30, the difference between a pixels has to be equal to or more than 30 to be marked changed. The higher this threshold, the more the pixels needs to be different to be marked.  pixel_threshold  defines a hard cut of in the number of pixels that need to be marked as changed before outputting motion values. This effectively sets the cutoff for noise when determining whether or not an infant is breathing. For example, if  pixel_threshold  is set to 100, the algorithm must see more than 100 pixels of change before registering any motion as seen. It will report no motion if that threshold is not crossed.  erode_dim  specifies the dimension of the kernel to use in an  OpenCV erode operation . This is used to minimize the changed pixels. That is, pixels that are isolated will be removed, but when large groups of pixels are changed, they will remain.  dilate_dim  is the opposite, it takes a pixels and expands it. These two parameters are used when detecting the region of the frame that the infant is located in. First, the pixel differences are calculated, then, a small erosion is applied to eliminate noise and a large dilation is applied to broadly mark the areas of motion.  low-cuttoff  and  high-cutoff  define the range of the bandpass filter used during magnification. Specifically, video magnification will try to magnify motion that occurs within this frequency range, and ignore motion outside this range. We've tuned this to be able to capture breathing rats in general, but you may need to tweak this during calibration.  See the section on calibration for more information.",
            "title": "Motion & Magnification"
        },
        {
            "location": "/setup/config/#debugging-features",
            "text": "The  show_diff  flag in  [motion]  will show a window where the areas where motion is detected in the frame are highlighted in white. The  show_magnification  flag in  [magnification]  controls a window that shows the output of just video magnification (which should look like the camera feed, in black and white, which enlarged motion).\nYou can use this two flags to show the result of your changes to the motion and magnification parameters.  Finally, the  print_times  in the  [debug]  section controls printing of frame times in the standard output, which you can use to calibrate the FPS and latency settings when running on a device different than the Raspberry Pi.  These features must be left to off when cribsense is started through systemd (automatically on boot or with  systemctl start ). They are only useful if you run cribsense manually.",
            "title": "Debugging features"
        },
        {
            "location": "/setup/config/#calibrating-the-motion-magnification-algorithm",
            "text": "Calibration of the algorithm is an iterative effort, with no right or wrong answer. We encourage you to experiment with various values, combining them with the debugging features, to find the combination of parameters most suitable to your environment.  As a guideline, increasing the  amplification  and the  phase_threshold  values increases the amount of magnification applied to the input video. You should change these values until you clearly see the movement from your baby breathing, and no significant artifact in other areas of the video. If you experience artifacts, reducing the  phase_threshold  while keeping the same  amplification  might help. You can view the effects of these parameters by setting  show_magnification  to  true .  As for the motion detection parameters, the main driver is the amount of noise. When detecting regions of motion to crop to,  erode_dim  and  dilate_dim  are used to size the dimensions of the OpenCV kernels used to  erode and dilate  motion so that noise is first eroded away, then the remaining motion signal is significantly dilated to make the regions of motion obvious. These parameters may also need to be tuned if your crib is in a very high-contrast setting. In general, you will need a higher  erode_dim  for high contrast settings, and a lower  erode_dim  for low contrast.  If you run with  show_diff = true  and you notice that too much of the input video is white, or some completely unrelated part of the video is detected as motion (e.g. a flickering lamp), you'll want to increase the  erode_dim  until only the part of the video corresponding to your breathing baby is the largest section of white. The top figure shows an example where the erode dimension is too low for the amount of motion in the frame, while the bottom one shows a correctly calibrated frame.    Once this has been calibrated, you'll want to make sure that the  pixel_threshold  is set so that motion is only reported when you expect, and not constantly (which means you need to cut out the noise). Ideally, you'll see output like this in your terminal:  [info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 44    [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 161   [info] Motion Estimate: 1.219812 Hz\n[info] Pixel Movement: 121   [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 86    [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 97    [info] Motion Estimate: 0.841416 Hz\n[info] Pixel Movement: 74    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 60    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 48    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 38    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 29    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 28    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 22    [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz\n[info] Pixel Movement: 0     [info] Motion Estimate: 0.839298 Hz  Where there is a clear periodic pattern corresponding to the motion.  If your output looks more like this:  [info] Pixel Movement: 921   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 736   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 666   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 663   [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 1196  [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 1235  [info] Motion Estimate: 1.352046 Hz\n[info] Pixel Movement: 1187  [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 1115  [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 959   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 744   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 611   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 468   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 371   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 307   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 270   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 234   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 197   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 179   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 164   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 239   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 733   [info] Motion Estimate: 1.456389 Hz\n[info] Pixel Movement: 686   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 667   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 607   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 544   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 499   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 434   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 396   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 375   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 389   [info] Motion Estimate: 1.229389 Hz\n[info] Pixel Movement: 305   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 269   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 1382  [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 1086  [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 1049  [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 811   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 601   [info] Motion Estimate: 1.312346 Hz\n[info] Pixel Movement: 456   [info] Motion Estimate: 1.312346 Hz  Then you'll need to adjust  pixel_threshold  and  diff_threshold  until just peaks are seen, and pixel movement is 0 otherwise.",
            "title": "Calibrating the Motion & Magnification algorithm"
        },
        {
            "location": "/setup/troubleshooting/",
            "text": "Troubleshooting\n\n\nHere are some troubleshooting tips we've gathered while making CribSense.\n\n\nNo alarm is playing\n\n\n\n\nAre your speakers working?\n\n\nCan you play other sounds from the Pi outside of the CribSense alarm?\n\n\nIs your Pi trying to play audio through HDMI rather than the audio port? Check the \nRaspberry Pi Audio Configuration\n page to make sure that you have selected the right output.\n\n\nIs CribSense detecting motion? Check with \njournalctl -f\n to see if CribSense is running in the background. If it is, perhaps you need to \ncalibrate CribSense\n\n\n\n\nCribSense is detecting motion even though there isn't an infant\n\n\n\n\nHave you properly \nconfigured CribSense\n?\n\n\nRemember, CribSense is just looking for changes in pixel values\n\n\nIs there a moving shadow within the frame?\n\n\nIs there flickering or changing lighting?\n\n\nIs CribSense mounted to a stable surface (e.g. something that won't shake if people are walking by it)?\n\n\nIs there any other sources of movement in the frame (mirrors catching reflections, etc)?\n\n\n\n\n\n\n\n\nCribSense is NOT detecting motion even though there is motion\n\n\n\n\nHave you properly \nconfigured CribSense\n?\n\n\nIs there anything in the way of the camera?\n\n\nAre you able to connect to the camera from the Raspberry Pi at all? Check by running \nraspistill -v\n in a terminal to open the camera on the Pi for a few seconds.\n\n\nIf you look at \nsudo systemctl status cribsense\n, is it actually running?\n\n\nIs your infant under a blanket that is \"tented\" up so that it is not making contact with the child? If there is significant gaps between the blanket and the child, this might mask the motion.\n\n\nCan you see the motion if you amplify the video more?\n\n\nCan you see the motion if you tune the low and high frequency cutoffs?\n\n\nIf this is happening in low-light only, did you make sure your calibration works in low-light?\n\n\n\n\nCribSense doesn't build\n\n\n\n\nDid you \ninstall all of the dependencies\n?\n\n\n\n\nI can't run \ncribsense\n from the commandline\n\n\n\n\nDid you accidentally mistype anything when you ran \n./autogen.sh --prefix=/usr --sysconfdir=/etc --disable-debug\n during your software build?\n\n\nIs \ncribsense\n present in \n/usr/bin/\n?",
            "title": "Troubleshooting"
        },
        {
            "location": "/setup/troubleshooting/#troubleshooting",
            "text": "Here are some troubleshooting tips we've gathered while making CribSense.  No alarm is playing   Are your speakers working?  Can you play other sounds from the Pi outside of the CribSense alarm?  Is your Pi trying to play audio through HDMI rather than the audio port? Check the  Raspberry Pi Audio Configuration  page to make sure that you have selected the right output.  Is CribSense detecting motion? Check with  journalctl -f  to see if CribSense is running in the background. If it is, perhaps you need to  calibrate CribSense   CribSense is detecting motion even though there isn't an infant   Have you properly  configured CribSense ?  Remember, CribSense is just looking for changes in pixel values  Is there a moving shadow within the frame?  Is there flickering or changing lighting?  Is CribSense mounted to a stable surface (e.g. something that won't shake if people are walking by it)?  Is there any other sources of movement in the frame (mirrors catching reflections, etc)?     CribSense is NOT detecting motion even though there is motion   Have you properly  configured CribSense ?  Is there anything in the way of the camera?  Are you able to connect to the camera from the Raspberry Pi at all? Check by running  raspistill -v  in a terminal to open the camera on the Pi for a few seconds.  If you look at  sudo systemctl status cribsense , is it actually running?  Is your infant under a blanket that is \"tented\" up so that it is not making contact with the child? If there is significant gaps between the blanket and the child, this might mask the motion.  Can you see the motion if you amplify the video more?  Can you see the motion if you tune the low and high frequency cutoffs?  If this is happening in low-light only, did you make sure your calibration works in low-light?   CribSense doesn't build   Did you  install all of the dependencies ?   I can't run  cribsense  from the commandline   Did you accidentally mistype anything when you ran  ./autogen.sh --prefix=/usr --sysconfdir=/etc --disable-debug  during your software build?  Is  cribsense  present in  /usr/bin/ ?",
            "title": "Troubleshooting"
        }
    ]
}